
--- 7/9/23 ---

== sorting algs ==
selection sort - insertion sort
both O(n^2), lower constant factor (?) than bubble sort

sel sort:
partitioning: unsorted, sorted
iter: select smallest, move to start of sorted

note that in each iteration, only one item is resolved: therfore, total iterations is some approximation of (n^2)/2

ins sort:
similar partitioning
iter over each elem: loop through sorted and insert in appropriate pos
can binary search be used to achieve n log n? | this does not work because insertion necessarily require full swaps

item will take on average n/2 checks to find appropriate pos: therfore also (n^2)/2
(note that bubble sort can be similarly bound-checked to approximate (n^2)/2 operations)


remember that i,j = j,i exists

== big O notation ==
metric we use to describe approximately how #ops scales with n, the size of the input parameter

analogy: file on hard drive, need to send to friend a sufficiently large distance away
optimizing parameter is time of transfer
ftp vs. physical delivery - bandwidth problem

1 -> log n -> n -> n log n -> n^2 -> 2^n

--- continue 11/9/23 ---

big o, big theta, big omega

print all array vals: O(n)
formally, defined (in academia) as upper bound of time for an alg - alg is at least as fast as this

for all arr val: O(n) - takes <= ops O(n) to print n items

however we want O(n) to be asymptotically tight

big omega:
in academia: defined as lower bound to time for algorithm (cannot be significantly faster than this)
could instead be log n, or 1, but n is best approx

big theta:
in academia: both O() and big_omega() - in an alg, big_theta(n) if and only if O(n) and also big_omega(n)
tighter asymptotic bound

In industry this is diff: O(), big_theta() are equiv.
industry meaning of O() is similar to academically big_theta()

in this class, despite academically, we will use industry O(), optimize for tightest def. of runtime

best case, expected case, worst case = big_omega(), big_theta(), O()
X no particular relation

apply this on quicksort

== space complexity ==
sometimes memory is the limiting factor rather than time

arr of size n: O(n) space
2d arr: O(n^2) space

stack space in a recursive call:
sum: n-> if n<0 ret 0; ret n+sum(n-1);

stack space of O(n), time of O(n)

ex2 | pair_sum_seq: n-> sum=0; for i 0->n: sum+=pair_sum(i,i+1); ret sum;
      pair_sum: a,b-> ret a+b;

time: O(n), stack: O(1)

dropping constants: O(n) code could run faster than O(1) for specific inputs: constant factor
O() only desc rate of increase, not the actual runtime

alg described as O(2n) is equiv to O(n)

dropping non-dominant terms: O(n^3 + n^2 + n) is equiv to O(n^3)

--- 13/9/23 ---

== merge sort ==

Subdivide the array into two sections, then recursively keep subdividing and merging these sections until sorted

merging: because both subarrays are sorted, create a sorted partition, and look at the leading members of each subarray
in turn until the whole array is finished

subrot: sort, merge

complexity: 3 visits per elem
if v(n) = time to sort array of len n, v(n) = v(n/2) + v(n/2) + 3n; or v(n)=2v(n/2)+3n
assume n=2^m: v(2^m)=2v(2^(m-1))+3n

Recursively we can derive that v(n) = 2^k v(n/(2^k)) + 3nk
therefore: defining a base case of v(1) = 1; v(n) = 2^(log n) + 3n(log n) -> n + 3n log n => O(n log n)
If v(1) = 0 (which is algorithmically possible) this will result in v(n) -> 3n log n

--- 27/9/23 ---

data types: a set of values and a set of operations that can be applied to said values

== abstraction ==

also encapsulation: construct that helps bundle data with methods (i.e. inline method instead of external func)

ADTs: data type whose representation gets hidden (?) from client - supports encapsulation in program design
	ex: do not need to know what bits do what / how datatype stores is implemented in order to use it - python dynamically allocated arrays

--- 29/9/23 ---

== hash tables ==

datastructure - maps keys to values for highly efficient lookup
number of ways to implement this | hash table / binary tree ?

simple implementation: array of linked lists
[
 & -> & -> ...
 & -> & -> ...
 ...
]
hashcode func: decompose any val into int/long rep

insertion: in -> key | hash_code(key) -> map hashcode to index in arr, and insert
however: collisions | because of this, allocate a new object and point to it w linked_list

retrieval: in -> key | hash(key)%len > map[index]; traverse ll until key; output value

prop of hash functions: take use of all info of key [ ex: do not use str[0] ]
hash values should be uniformly distributed
similar keys should output very different results
hash func must be fast

--- 17/10/23 ---

== images ==

processing stuff idk


--- 27/11/23 ---

== ADTs ==

abstract data types

== linked lists ==
	later: stacks, queues, trees, graphs

made up of nodes: node -> { value, next, [previous] }

singly-linked lists:
	*head -> first node -> second node -> etc. -> last node -> nil

iterating over ll:
	start at *head; call next() until next == nil

*cannot randomly access arbitrary element n at its direct memory location; must iterate
	additionally cannot traverse backwards

doubly-linked lists: each node stores *next, *previous
	*head -> first node <-> second <-> etc. <-> last -> nil
                     v
                    nil
ops:
	build:
		node *current;
		void alloc(string value){
			current->next = new node();
			current = current->next;
			current->value = value;
			current->next = nil;
		}
		node first; first.value = "W"; first.next = nil;
		current = &first;
		alloc("T");
		alloc("Rocks!");

	insert @ begin:
		// pre: node *head, string val
		node new; new.value = val;
		new.next = head;
		next = &new;

	remove @ begin:
		// pre: node *head
		node *tmp = head->next;
		delete head->next;
		head = tmp;

	insert at end:
		// pre: node *head, string val
		node *tmp = head;
		while(tmp->next != nil) tmp = tmp->next;
		tmp->next = new node();
		(tmp->next)->value = val; (tmp->next)->next = nil;

	insert at n:
		// pre: node *head, int n, string val
		node *tmp = head;
		while(n-->0) tmp = tmp->next;
		node *ptr = tmp;
		tmp->next = new node();
		(tmp->next)->value = val; (tmp->next)->next = ptr;

--- 29/11/23 ---

== stacks ==

usually implemented with linked list ?

last in first out queue structure

push/pop ops

implementation:

ops:
	Stack(): creates new stack

	push(item): pushes item to stack

	pop(): returns topmost item in stack, and pops

	peek(): returns topmost item, does not modify

	isEmpty(): does what it looks like it does

	size(): read above

s = Stack()
op               stack              ret
s.isEmpty()     {}                  True
s.push(4)       {4}                 null
s.push("dog")   {4,"dog"}           null
s.peek()        {4,"dog"}           "dog"
s.push(True)    {4,"dog",True}      null
s.size()        {4,"dog",True}      3
s.isEmpty()     {4,"dog",True}      False
s.push(8.4)     {4,"dog",True,8.4}  null
s.pop()         {4,"dog",True}      8.4
s.pop()         {4,"dog"}           True
s.size()        {4,"dog"}           2

--- 7/12/23 ---

== queues ==

similar to stack: uses internal ll

ops:
	Queue(): ctor

	enqueue(item): insert into end

	dequeue(): return last item & pop

	size(): self explanatory
	isEmpty(): ^

op                   contents            ret
isEmpty           {}                     True
enqueue(4)        {4}
enqueue('dog')    {4,'dog'}
enqueue(True)     {4,'dog',True}
size              {4,'dog',True}         3
isEmpty           {4,'dog',True}         False
enqueue(8.4)      {4,'dog',True,8.4}
dequeue           {'dog',True,8.4}       4
dequeue           {True,8.4}             'dog'
size              {True,8.4}             2
